{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"openQA","text":"<p>openQA is an open-source framework designed to automate the testing and evaluation of AI models. It provides a comprehensive architecture for defining audit configurations, running tests, analyzing results, and generating reports.  </p> <p>Traditionally, testing AI models involves complex, subjective evaluations. AI Auditor changes that by introducing a revolutionary approach - leveraging a second Large Language Model (LLM) for distance-based scoring. This innovative framework empowers you to:  </p> <ul> <li>Automate AI testing: Define clear audit configurations with pre-defined and programmatic inputs/outputs for consistent evaluation.  </li> <li>Achieve objective scoring: The second LLM objectively assesses the discrepancy between the AI model's output and the desired outcome, eliminating human bias.  </li> <li>Gain deeper insights: Generate detailed reports highlighting identified issues, distance scores, and areas for improvement.  </li> </ul> <p>AI Auditor goes beyond basic testing, providing a comprehensive and reliable solution for building trust in your AI models.   </p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Configurable Audits: Define audits with pre-defined and programmatic inputs/outputs for flexible testing scenarios.  </li> <li>Modular Architecture : Leverage separate components for configuration management, test execution, evaluation, and reporting.  </li> <li>Distance-Based Scoring : Employ a second LLM to calculate the distance between desired and actual outputs for objective scoring.  </li> <li>Detailed Reports : Generate comprehensive reports summarizing audit results, identified discrepancies, and scores.  </li> </ul>"},{"location":"#architecture","title":"Architecture:","text":"<p>The core architecture of AI Auditor consists of several interacting components:  </p> <ul> <li>Audit Config: This component defines the configuration for a specific audit. It includes:</li> <li>Pre-defined inputs: Specific data or prompts to be fed to the AI model under test.</li> <li>Programmatic input generation: Python or similar code to dynamically generate inputs based on specific criteria.</li> <li>Desired outputs: Expected outputs from the AI model for the provided inputs. These can be pre-defined text, data structures, or scoring criteria.</li> <li>Configuration Management: This component manages and stores audit configurations, allowing for easy creation, modification, and version control.</li> <li>Runner: This component executes the AI model under test according to the specified configuration. It provides the defined inputs to the model and captures the generated outputs.</li> <li>LLM Evaluator: This component utilizes a second Large Language Model (LLM) to compare the AI model's outputs with the desired outputs from the configuration. It calculates a distance score based on the closeness of the outputs, indicating potential discrepancies.</li> <li>Reporting: This component generates comprehensive reports summarizing the audit results. It includes:  </li> <li>Tested AI model and configuration details.  </li> <li>Pre-defined and programmatic inputs used.  </li> <li>Desired outputs from the configuration.  </li> <li>Actual outputs generated by the AI model under test.  </li> <li>Distance score calculated by the LLM Evaluator.  </li> <li>Identified discrepancies or areas for improvement.  </li> </ul> <p></p>"},{"location":"#benefits","title":"Benefits:","text":"<ul> <li>Standardizes and simplifies AI model testing procedures.</li> <li>Enables objective and consistent evaluation through distance-based scoring.</li> <li>Improves transparency and interpretability of AI model behavior.</li> <li>Generates detailed reports for informed decision-making.</li> </ul>"}]}